{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242a7e82",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710a6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class preprocessor:\n",
    "    \n",
    "    \n",
    "    def remove_columns(data,columns):\n",
    "        # taking dataframe and name of columns to be removed\n",
    "        \n",
    "        useful_data=data.drop(labels=columns,axis=1)\n",
    "        \n",
    "        return useful_data\n",
    "    \n",
    "    def separate_label_feature(data,label_column_name):\n",
    "        #taking dataframe and label col name or output col name\n",
    "        X = data.drop(labels=label_column_name,axis=1)\n",
    "        Y = data[label_column_name] #take the output col in Y\n",
    "        \n",
    "        return X,Y\n",
    "    \n",
    "    def dropUnnecessaryCol(data,columnList):\n",
    "        data = data.drop(columnList,axis=1)\n",
    "        return data\n",
    "    \n",
    "    def standardScaling(X):\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8049a",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41cf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "class KMeansClustering:\n",
    "    \n",
    "    def elbow_plot(data):\n",
    "        ss=[]\n",
    "        \n",
    "        for i in range(1,11):\n",
    "            kmeans=KMeans(n_clusters=i,init='k-means++',random_state=7)\n",
    "            kmeans.fit(data)\n",
    "            ss.append(kmeans.inertia_)\n",
    "            \n",
    "        plt.plot(range(1,11),ss) # creating the graph between SS and the number of clusters\n",
    "        plt.title('The Elbow Method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('SS')\n",
    "        \n",
    "        plt.savefig('K-Means_Elbow.PNG')\n",
    "        \n",
    "        kn = KneeLocator(range(1, 11), ss, curve='convex', direction='decreasing') #Xaxis,Yaxis,curveshape,direction\n",
    "        \n",
    "        return kn.knee\n",
    "    \n",
    "    def create_cluster(data,number_of_clusters):\n",
    "        kmeans = KMeans(n_clusters=number_of_clusters, init='k-means++', random_state=7)\n",
    "        \n",
    "        y_kmeans = kmeans.fit_predict(data)  #divide data into clusters\n",
    "        save_model = modelOperation.save_model(kmeans, 'KMeans')\n",
    "        \n",
    "        data['Cluster']=y_kmeans  # create a new column in dataset for storing the cluster information\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f307e",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aabdf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class modelOperation:\n",
    "    model_dir = \"D:/cdac/CDAC_PROJECT/Untitled Folder/model\"\n",
    "        \n",
    "    def save_model(model,filename):\n",
    "        path = os.path.join(\"D:/cdac/CDAC_PROJECT/Untitled Folder/model\",filename)  #create seperate directory for each cluster\n",
    "        if os.path.isdir(path):   #remove previously existing models for each clusters\n",
    "            shutil.rmtree(model_dir)\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            os.makedirs(path) \n",
    "            with open(path +'/' + filename+'.sav','wb') as f:\n",
    "                \n",
    "                pickle.dump(model, f)\n",
    "        return 'success'\n",
    "    \n",
    "    def load_model(filename):\n",
    "        with open(model_dir + filename + '/' + filename + '.sav','rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def find_correct_model_file(cluster_number):\n",
    "        \n",
    "        cluster_number= cluster_number\n",
    "        folder_name= model_dir\n",
    "        list_of_model_files = []\n",
    "        list_of_files = os.listdir(folder_name)\n",
    "        for file in list_of_files:\n",
    "            try:\n",
    "                if (file.index(str(cluster_number))!=-1):\n",
    "                    model_name=file\n",
    "            except:\n",
    "                continue\n",
    "        model_name=model_name.split('.')[0]\n",
    "        return model_name\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa72362",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781fb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97b7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data_validation:\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        self.Batch_Directory = path\n",
    "        self.schema_path = 'schema.json'\n",
    "    def valuesFromSchema(self):\n",
    "        # it will extract all the information from given schema\n",
    "        f = open(self.schema_path,'r')\n",
    "        dict1 = json.load(f)  # it will return json object containing data in key-value pairs\n",
    "        f.close()\n",
    "        #print(type(dict1))\n",
    "        pattern = dict1['SampleFileName']\n",
    "        LengthOfDateStampInFile = dict1['LengthOfDateStampInFile']\n",
    "        LengthOfTimeStampInFile = dict1['LengthOfTimeStampInFile']\n",
    "        column_names = dict1['ColName']\n",
    "        NumberofColumns = dict1['NumberofColumns']\n",
    "        \n",
    "        return LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, NumberofColumns\n",
    "    \n",
    "    def FileNameRegex(self):\n",
    "        #regular exp for the filename from training batch files\n",
    "        regex = \"['visibility']+['\\_'']+[\\d_]+[\\d]+\\.csv\"\n",
    "        return regex\n",
    "    def validationFileName(self,regex,LengthOfDateStampInFile,LengthOfTimeStampInFile):\n",
    "        # file name validation with regex and schema info\n",
    "        \n",
    "        onlyfiles = [f for f in listdir(self.Batch_Directory)] # this will give all the files at given path\n",
    "        destination=\"Training_files_validated/Good_data\"\n",
    "        destination2=\"Training_files_validated/Bad_data\"\n",
    "        for filename in onlyfiles:\n",
    "                if (re.match(regex, filename)):\n",
    "                    split1 = re.split('.csv', filename)\n",
    "                    split2 = (re.split('_', split1[0]))\n",
    "                    if len(split2[1]) == LengthOfDateStampInFile:\n",
    "                        \n",
    "                        if len(split2[2]) == LengthOfTimeStampInFile:\n",
    "                            shutil.copy(\"Training_Batch_Files/\" + filename, \"Training_files_validated/Good_data\")\n",
    "                        else:\n",
    "                            shutil.copy(\"Training_Batch_Files/\" + filename, \"Training_files_validated/Bad_data\")\n",
    "                    else:\n",
    "                        shutil.copy(\"Training_Batch_Files/\" + filename, \"Training_files_validated/Bad_data\")\n",
    "                else:\n",
    "                    shutil.copy(\"Training_Batch_Files/\" + filename, \"Training_files_validated/Bad_data\")\n",
    "        \n",
    "    def validateColumnLength(self,NumberofColumns):\n",
    "        # even if file name is right ,it may happen that no. of cols are not same\n",
    "        # so this function will validate that.\n",
    "        \n",
    "        for file in listdir('Training_files_validated/Good_data/'):\n",
    "            csv = pd.read_csv(\"Training_files_validated/Good_data/\" + file)\n",
    "            if csv.shape[1] == NumberofColumns:  #shape gives(rows,columns) so index 1\n",
    "                pass\n",
    "            else:\n",
    "                shutil.move(\"Training_files_validated/Good_data/\" + file, \"Training_files_validated/Bad_data\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b5531",
   "metadata": {},
   "source": [
    "## Database Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6cc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "790df995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dBOperation:           #This class is used for handling all the SQL operations.\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.badFilePath = \"Training_files_validated/Bad_data\"\n",
    "        self.goodFilePath = \"Training_files_validated/Good_data\"\n",
    "    def dataBaseConnection(self,DatabaseName):\n",
    "        # This method creates the database with the given name and \n",
    "        # if Database already exists then opens the connection to the DB.\n",
    "        \n",
    "        conn = sqlite3.connect(DatabaseName+'.db')\n",
    "        return conn\n",
    "    \n",
    "    def createTableDb(self,DatabaseName,column_names):\n",
    "        \n",
    "    #This method creates a table in the given database which will be used to insert the Good data\n",
    "        conn = self.dataBaseConnection(DatabaseName)\n",
    "        for key in column_names.keys():\n",
    "            type = column_names[key]\n",
    "            try:\n",
    "                conn.execute('ALTER TABLE Good_Raw_Data ADD COLUMN \"{column_name}\" {dataType}'.format(column_name=key,dataType=type))\n",
    "            except:\n",
    "                \n",
    "                conn.execute('CREATE TABLE  Good_Raw_Data ({column_name} {dataType})'.format(column_name=key, dataType=type))\n",
    "        conn.close()\n",
    "        \n",
    "    def insertIntoTableGoodData(self,Database):\n",
    "        conn = self.dataBaseConnection(Database)\n",
    "        goodFilePath= self.goodFilePath\n",
    "        badFilePath = self.badFilePath\n",
    "        onlyfiles = [f for f in listdir(goodFilePath)]\n",
    "        \n",
    "        for file in onlyfiles:\n",
    "            \n",
    "            with open(goodFilePath+'/'+file, \"r\") as f:\n",
    "                next(f)\n",
    "                reader = csv.reader(f, delimiter=\"\\n\")\n",
    "                for line in enumerate(reader):\n",
    "                    for list_ in (line[1]):\n",
    "                        conn.execute('INSERT INTO Good_Raw_Data values ({values})'.format(values=(list_)))\n",
    "                        conn.commit()\n",
    "                \n",
    "\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "            \n",
    "    def selectingDatafromtableintocsv(self,Database):\n",
    "        #This method exports the data from Good_Raw_Data table as a CSV file. at a given location.\n",
    "        \n",
    "        \n",
    "        self.fileName = 'InputFile.csv'\n",
    "        conn = self.dataBaseConnection(Database)\n",
    "        sqlSelect = \"SELECT *  FROM Good_Raw_Data\"\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(sqlSelect)\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        # Get the headers of the csv file\n",
    "        headers = [i[0] for i in cursor.description]  #description property will return a list of tuples describing the columns\n",
    "        # 0th index is always a col name in description \n",
    "        with open( self.fileName, 'w', newline='') as csvFile:\n",
    "            csvFile = csv.writer(csvFile,delimiter=',',lineterminator='\\n')\n",
    "        \n",
    "            csvFile.writerow(headers)   # for single row at a time--to write the field names or col names\n",
    "            csvFile.writerows(results)  # for multiple rows at a time\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c25e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_validation:\n",
    "    def __init__(self,path):\n",
    "        self.raw_data = Raw_Data_validation(path)\n",
    "        self.dBOperation = dBOperation()\n",
    "        \n",
    "    def train_validation(self):\n",
    "        \n",
    "            # extracting values from prediction schema\n",
    "            LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, noofcolumns = self.raw_data.valuesFromSchema()\n",
    "            # getting the regex defined to validate filename\n",
    "            regex = self.raw_data.FileNameRegex()\n",
    "            # validating filename of prediction files\n",
    "            self.raw_data.validationFileName(regex, LengthOfDateStampInFile, LengthOfTimeStampInFile)\n",
    "            # validating column length in the file\n",
    "            self.raw_data.validateColumnLength(noofcolumns)\n",
    "            \n",
    "           \n",
    "            # create database with given name, if present open the connection! Create table with columns given in schema\n",
    "            self.dBOperation.createTableDb('Training', column_names)\n",
    "           \n",
    "            # insert csv files in the table\n",
    "            self.dBOperation.insertIntoTableGoodData('Training')\n",
    "            \n",
    "\n",
    "            # export data in table to csvfile\n",
    "            self.dBOperation.selectingDatafromtableintocsv('Training')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66485ad6",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30d7a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics  import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3a3e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Finder:\n",
    "    def __init__(self):\n",
    "        self.clf = RandomForestClassifier()\n",
    "        self.DecisionTreeReg = DecisionTreeRegressor()\n",
    "    \n",
    "    def get_best_params_for_random_forest(self,train_x,train_y):\n",
    "            #get the parameters for Random Forest Algorithm which give the best accuracy.\n",
    "            #Use Hyper Parameter Tuning.\n",
    "            # output will be The model with the best parameters\n",
    "            \n",
    "            # initializing with different combination of parameters\n",
    "            self.param_grid = {\"n_estimators\": [10, 50, 100, 130], \"criterion\": ['gini', 'entropy'],\n",
    "                               \"max_depth\": range(2, 4, 1), \"max_features\": ['auto', 'log2']}\n",
    "            \n",
    "            #Creating an object of the Grid Search class\n",
    "            self.grid = GridSearchCV(estimator=self.clf, param_grid=self.param_grid, cv=5)\n",
    "            \n",
    "            self.grid.fit(train_x, train_y)\n",
    "            \n",
    "            #extracting the best parameters\n",
    "            self.criterion = self.grid.best_params_['criterion']\n",
    "            self.max_depth = self.grid.best_params_['max_depth']\n",
    "            self.max_features = self.grid.best_params_['max_features']\n",
    "            self.n_estimators = self.grid.best_params_['n_estimators']\n",
    "            \n",
    "            #creating a new model with the best parameters\n",
    "            self.clf = RandomForestClassifier(n_estimators=self.n_estimators, criterion=self.criterion,\n",
    "                                              max_depth=self.max_depth, max_features=self.max_features)\n",
    "            # training the mew model\n",
    "            self.clf.fit(train_x, train_y)\n",
    "            \n",
    "            return self.clf\n",
    "        \n",
    "        ###### for Decision Tree  ############\n",
    "        \n",
    "    def get_best_params_for_DecisionTreeRegressor(self, train_x, train_y):\n",
    "        #get the parameters for DecisionTreeRegressor Algorithm which gives best accuracy\n",
    "        #Output will be The model with the best parameters\n",
    "        \n",
    "        self.param_grid_decisionTree = {\"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "                              \"splitter\": [\"best\", \"random\"],\n",
    "                              \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                              'max_depth': range(2, 16, 2),\n",
    "                              'min_samples_split': range(2, 16, 2)\n",
    "                              }\n",
    "        # Creating an object of the Grid Search class\n",
    "        self.grid = GridSearchCV(self.DecisionTreeReg, self.param_grid_decisionTree, verbose=3,cv=5)\n",
    "        \n",
    "        self.grid.fit(train_x, train_y)\n",
    "\n",
    "        # extracting the best parameters\n",
    "        self.criterion = self.grid.best_params_['criterion']\n",
    "        self.splitter = self.grid.best_params_['splitter']\n",
    "        self.max_features = self.grid.best_params_['max_features']\n",
    "        self.max_depth  = self.grid.best_params_['max_depth']\n",
    "        self.min_samples_split = self.grid.best_params_['min_samples_split']\n",
    "        \n",
    "        # creating a new model with the best parameters\n",
    "        self.decisionTreeReg = DecisionTreeRegressor(criterion=self.criterion,splitter=self.splitter,\n",
    "                                                     max_features=self.max_features,max_depth=self.max_depth,\n",
    "                                                     min_samples_split=self.min_samples_split)\n",
    "        # training the new models\n",
    "        self.decisionTreeReg.fit(train_x, train_y)\n",
    "        \n",
    "        return self.decisionTreeReg\n",
    "    \n",
    "    def get_best_params_for_xgboost(self,train_x,train_y):\n",
    "        #get the parameters for XGBoost Algorithm which give the best accuracy.Use Hyper Parameter Tuning.\n",
    "        #output will be The model with the best parameters\n",
    "        \n",
    "        # initializing with different combination of parameters\n",
    "        self.param_grid_xgboost = {\n",
    "                            'learning_rate': [0.5, 0.1, 0.01, 0.001],\n",
    "                            'max_depth': [3, 5, 10, 20],\n",
    "                            'n_estimators': [10, 50, 100, 200]\n",
    "                            }\n",
    "        self.grid= GridSearchCV(XGBRegressor(objective='reg:linear'),self.param_grid_xgboost, verbose=3,cv=5)\n",
    "        \n",
    "        self.grid.fit(train_x, train_y)\n",
    "        \n",
    "        # extracting the best parameters\n",
    "        self.learning_rate = self.grid.best_params_['learning_rate']\n",
    "        self.max_depth = self.grid.best_params_['max_depth']\n",
    "        self.n_estimators = self.grid.best_params_['n_estimators']\n",
    "        \n",
    "        # creating a new model with the best parameters\n",
    "        self.xgb = XGBRegressor(objective='reg:linear',learning_rate=self.learning_rate, \n",
    "                                max_depth=self.max_depth, n_estimators=self.n_estimators)\n",
    "        \n",
    "        # training the mew model\n",
    "        self.xgb.fit(train_x, train_y)\n",
    "        \n",
    "        return self.xgb\n",
    "    \n",
    "    def get_best_model(self,train_x,train_y,test_x,test_y):\n",
    "            #Find out the Model which has the best AUC score.\n",
    "            #Output: The best model name and the model object\n",
    "            ## for decision tree\n",
    "            self.decisionTreeReg= self.get_best_params_for_DecisionTreeRegressor(train_x, train_y)\n",
    "            self.prediction_decisionTreeReg = self.decisionTreeReg.predict(test_x) # Predictions using the decisionTreeReg Model\n",
    "            self.decisionTreeReg_error = r2_score(test_y,self.prediction_decisionTreeReg)\n",
    "            \n",
    "            # create best model for XGBoost\n",
    "            self.xgboost = self.get_best_params_for_xgboost(train_x, train_y)\n",
    "            self.prediction_xgboost = self.xgboost.predict(test_x)  # Predictions using the XGBoost Model\n",
    "            self.prediction_xgboost_error = r2_score(test_y,self.prediction_xgboost)\n",
    "            \n",
    "             #comparing the two models\n",
    "            if(self.decisionTreeReg_error <  self.prediction_xgboost_error):\n",
    "                return 'XGBoost',self.xgboost\n",
    "            else:\n",
    "                return 'DecisionTreeReg',self.decisionTreeReg\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d64c962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import preprocessor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cb4e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/cdac/CDAC_PROJECT/Untitled Folder/Training_Batch_Files/visibility_08012008_120010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e3b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainModel:\n",
    "    model_dir = \"model/\"\n",
    "    \n",
    "    def trainingModel():\n",
    "            \n",
    "        ### preprocessing #####\n",
    "        #removing unwanted columns as seen in the EDA part\n",
    "        data = preprocessor.dropUnnecessaryCol(data,['DATE','Precip','WETBULBTEMPF','DewPointTempF','StationPressure'])\n",
    "\n",
    "        # create separate features and labels\n",
    "        X, Y = preprocessor.separate_label_feature(data, label_column_name='VISIBILITY')\n",
    "\n",
    "        #kmeans=KMeansClustering() # object initialization.\n",
    "        number_of_clusters=KMeansClustering.elbow_plot(X)  #  using the elbow plot to find the number of optimum clusters\n",
    "\n",
    "        # Divide the data into clusters\n",
    "        X = KMeansClustering.create_cluster(X,number_of_clusters)\n",
    "\n",
    "        #create a new column in the dataset consisting of the corresponding cluster assignments.\n",
    "        X['Labels']=Y\n",
    "\n",
    "         # getting the unique clusters from our dataset\n",
    "        list_of_clusters=X['Cluster'].unique()\n",
    "\n",
    "        ##parsing all the clusters and looking for the best ML algorithm to fit on individual cluster\n",
    "\n",
    "        for i in list_of_clusters:\n",
    "            cluster_data=X[X['Cluster']==i] # filter the data for one cluster\n",
    "\n",
    "            # Prepare the feature and Label columns\n",
    "            cluster_features=cluster_data.drop(['Labels','Cluster'],axis=1)\n",
    "            cluster_label= cluster_data['Labels']\n",
    "\n",
    "            # splitting the data into training and test set for each cluster one by one\n",
    "            x_train, x_test, y_train, y_test = train_test_split(cluster_features, cluster_label, test_size=0.3, random_state=7)\n",
    "\n",
    "            x_train_scaled = preprocessor.standardScaling(x_train)\n",
    "            x_test_scaled = preprocessor.standardScaling(x_test)\n",
    "\n",
    "            model_finder=Model_Finder()\n",
    "\n",
    "            #getting the best model for each of the clusters\n",
    "            best_model_name,best_model=model_finder.get_best_model(x_train_scaled,y_train,x_test_scaled,y_test)\n",
    "\n",
    "            #saving the best model to the directory.\n",
    "            #file_op = modelOperation()\n",
    "            save_model=modelOperation.save_model(best_model,best_model_name+str(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834f20b",
   "metadata": {},
   "source": [
    "## Prediction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89d10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction_Data_validation:\n",
    "    \n",
    "    def __init__(path):\n",
    "        Batch_Directory = path\n",
    "        schema_path = 'schema_prediction.json'\n",
    "    def valuesFromSchema():\n",
    "        # it will extract all the information from given schema\n",
    "        f = open(schema_path,'r')\n",
    "        dict1 = json.load(f)  # it will return json object containing data in key-value pairs\n",
    "        f.close()\n",
    "        #print(type(dict1))\n",
    "        pattern = dict1['SampleFileName']\n",
    "        LengthOfDateStampInFile = dict1['LengthOfDateStampInFile']\n",
    "        LengthOfTimeStampInFile = dict1['LengthOfTimeStampInFile']\n",
    "        column_names = dict1['ColName']\n",
    "        NumberofColumns = dict1['NumberofColumns']\n",
    "        \n",
    "        return LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, NumberofColumns\n",
    "    \n",
    "    def FileNameRegex():\n",
    "        #regular exp for the filename from training batch files\n",
    "        regex = \"['visibility']+['\\_'']+[\\d_]+[\\d]+\\.csv\"\n",
    "        return regex\n",
    "    def validationFileName(regex,LengthOfDateStampInFile,LengthOfTimeStampInFile):\n",
    "        # file name validation with regex and schema info\n",
    "        \n",
    "        onlyfiles = [f for f in listdir(Batch_Directory)] # this will give all the files at given path\n",
    "        #destination=\"Prediction_files_validated/Good_data\"\n",
    "        #destination2=\"Prediction_files_validated/Bad_data\"\n",
    "        for filename in onlyfiles:\n",
    "                if (re.match(regex, filename)):\n",
    "                    split1 = re.split('.csv', filename)\n",
    "                    split2 = (re.split('_', split1[0]))\n",
    "                    if len(split2[1]) == LengthOfDateStampInFile:\n",
    "                        \n",
    "                        if len(split2[2]) == LengthOfTimeStampInFile:\n",
    "                            shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Good_data\")\n",
    "                        else:\n",
    "                            shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "                    else:\n",
    "                        shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "                else:\n",
    "                    shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "        \n",
    "    def validateColumnLength(NumberofColumns):\n",
    "        # even if file name is right ,it may happen that no. of cols are not same\n",
    "        # so this function will validate that.\n",
    "        \n",
    "        for file in listdir('Prediction_files_validated/Good_data/'):\n",
    "            csv = pd.read_csv(\"Prediction_files_validated/Good_data/\" + file)\n",
    "            if csv.shape[1] == NumberofColumns:  #shape gives(rows,columns) so index 1\n",
    "                pass\n",
    "            else:\n",
    "                shutil.move(\"Prediction_files_validated/Good_data/\" + file, \"Prediction_files_validated/Bad_data\")\n",
    "                \n",
    "    def deletePredictionFile():\n",
    "\n",
    "        if os.path.exists('Prediction_Output_File/Predictions.csv'):\n",
    "            os.remove('Prediction_Output_File/Predictions.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d91c7",
   "metadata": {},
   "source": [
    "## Database Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c478b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dBOperationPredict:           #This class is used for handling all the SQL operations.\n",
    "    def __init__(self): \n",
    "        self.path = \"Prediction_Database/\"\n",
    "        self.badFilePath = \"Training_files_validated/Bad_data\"\n",
    "        self.goodFilePath = \"Training_files_validated/Good_data\"\n",
    "    \n",
    "    def dataBaseConnectionPredict(DatabaseName):\n",
    "        # This method creates the database with the given name and \n",
    "        # if Database already exists then opens the connection to the DB.\n",
    "        \n",
    "        conn = sqlite3.connect(self.path+DatabaseName+'.db')\n",
    "        return conn\n",
    "    \n",
    "    def createTableDbPredict(DatabaseName,column_names):\n",
    "        \n",
    "    #This method creates a table in the given database which will be used to insert the Good data\n",
    "        conn = dataBaseConnection(DatabaseName)\n",
    "        for key in column_names.keys():\n",
    "            type = column_names[key]\n",
    "            try:\n",
    "                conn.execute('ALTER TABLE Good_Raw_Data ADD COLUMN \"{column_name}\" {dataType}'.format(column_name=key,dataType=type))\n",
    "            except:\n",
    "                \n",
    "                conn.execute('CREATE TABLE  Good_Raw_Data ({column_name} {dataType})'.format(column_name=key, dataType=type))\n",
    "        conn.close()\n",
    "        \n",
    "    def insertIntoTableGoodDataPredict(Database):\n",
    "        conn = dataBaseConnection(Database)\n",
    "        goodFilePath= self.goodFilePath\n",
    "        badFilePath = self.badFilePath\n",
    "        onlyfiles = [f for f in listdir(goodFilePath)]\n",
    "        \n",
    "        for file in onlyfiles:\n",
    "            \n",
    "            with open(goodFilePath+'/'+file, \"r\") as f:\n",
    "                next(f)\n",
    "                reader = csv.reader(f, delimiter=\"\\n\")\n",
    "                for line in enumerate(reader):\n",
    "                    for list_ in (line[1]):\n",
    "                        conn.execute('INSERT INTO Good_Raw_Data values ({values})'.format(values=(list_)))\n",
    "                        conn.commit()\n",
    "                \n",
    "\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "            \n",
    "    def selectingDatafromtableintocsvPredict(Database):\n",
    "        #This method exports the data from Good_Raw_Data table as a CSV file. at a given location.\n",
    "        \n",
    "        \n",
    "        fileName = 'InputFile1.csv'\n",
    "        conn = dataBaseConnection(Database)\n",
    "        sqlSelect = \"SELECT *  FROM Good_Raw_Data\"\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(sqlSelect)\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        # Get the headers of the csv file\n",
    "        headers = [i[0] for i in cursor.description]  #description property will return a list of tuples describing the columns\n",
    "        # 0th index is always a col name in description \n",
    "        with open( fileName, 'w', newline='') as csvFile:\n",
    "            csvFile = csv.writer(csvFile,delimiter=',',lineterminator='\\n')\n",
    "        \n",
    "            csvFile.writerow(headers)   # for single row at a time--to write the field names or col names\n",
    "            csvFile.writerows(results)  # for multiple rows at a time\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aebe582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_validation:\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        Batch_Directory = path\n",
    "        schema_path = 'schema_prediction.json'\n",
    "        self.dbOperationPredict=dbOperationPredict()\n",
    "        \n",
    "    def valuesFromSchema():\n",
    "        # it will extract all the information from given schema\n",
    "        f = open(schema_path,'r')\n",
    "        dict1 = json.load(f)  # it will return json object containing data in key-value pairs\n",
    "        f.close()\n",
    "        #print(type(dict1))\n",
    "        pattern = dict1['SampleFileName']\n",
    "        LengthOfDateStampInFile = dict1['LengthOfDateStampInFile']\n",
    "        LengthOfTimeStampInFile = dict1['LengthOfTimeStampInFile']\n",
    "        column_names = dict1['ColName']\n",
    "        NumberofColumns = dict1['NumberofColumns']\n",
    "        \n",
    "        return LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, NumberofColumns\n",
    "    \n",
    "    def FileNameRegex():\n",
    "        #regular exp for the filename from training batch files\n",
    "        regex = \"['visibility']+['\\_'']+[\\d_]+[\\d]+\\.csv\"\n",
    "        return regex\n",
    "    def validationFileName(regex,LengthOfDateStampInFile,LengthOfTimeStampInFile):\n",
    "        # file name validation with regex and schema info\n",
    "        \n",
    "        onlyfiles = [f for f in listdir(Batch_Directory)] # this will give all the files at given path\n",
    "        #destination=\"Prediction_files_validated/Good_data\"\n",
    "        #destination2=\"Prediction_files_validated/Bad_data\"\n",
    "        for filename in onlyfiles:\n",
    "                if (re.match(regex, filename)):\n",
    "                    split1 = re.split('.csv', filename)\n",
    "                    split2 = (re.split('_', split1[0]))\n",
    "                    if len(split2[1]) == LengthOfDateStampInFile:\n",
    "                        \n",
    "                        if len(split2[2]) == LengthOfTimeStampInFile:\n",
    "                            shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Good_data\")\n",
    "                        else:\n",
    "                            shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "                    else:\n",
    "                        shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "                else:\n",
    "                    shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "        \n",
    "    def validateColumnLength(NumberofColumns):\n",
    "        # even if file name is right ,it may happen that no. of cols are not same\n",
    "        # so this function will validate that.\n",
    "        \n",
    "        for file in listdir('Prediction_files_validated/Good_data/'):\n",
    "            csv = pd.read_csv(\"Prediction_files_validated/Good_data/\" + file)\n",
    "            if csv.shape[1] == NumberofColumns:  #shape gives(rows,columns) so index 1\n",
    "                pass\n",
    "            else:\n",
    "                shutil.move(\"Prediction_files_validated/Good_data/\" + file, \"Prediction_files_validated/Bad_data\")\n",
    "                \n",
    "    def deletePredictionFile():\n",
    "\n",
    "        if os.path.exists('Prediction_Output_File/Predictions.csv'):\n",
    "            os.remove('Prediction_Output_File/Predictions.csv')\n",
    "            \n",
    "        \n",
    "    def prediction_validation():\n",
    "        \n",
    "            # extracting values from prediction schema\n",
    "            LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, noofcolumns = valuesFromSchema()\n",
    "            # getting the regex defined to validate filename\n",
    "            regex = FileNameRegex()\n",
    "            # validating filename of prediction files\n",
    "            validationFileName(regex, LengthOfDateStampInFile, LengthOfTimeStampInFile)\n",
    "            # validating column length in the file\n",
    "            validateColumnLength(noofcolumns)\n",
    "            \n",
    "           \n",
    "            # create database with given name, if present open the connection! Create table with columns given in schema\n",
    "            self.dBOperationPredict.createTableDbPredict('Training', column_names)\n",
    "           \n",
    "            # insert csv files in the table\n",
    "            self.dBOperationPredict.insertIntoTableGoodDataPredict('Training')\n",
    "            \n",
    "\n",
    "            # export data in table to csvfile\n",
    "            self.dBOperationPredict.selectingDatafromtableintocsvPredict('Training')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c4173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction:\n",
    "    def __init__(path):\n",
    "        pred_data_val = Prediction_Data_validation(path)\n",
    "    \n",
    "    def predictionFromModel():\n",
    "        data = preprocessor.dropUnnecessaryCol(data,['DATE','Precip','WETBULBTEMPF','DewPointTempF','StationPressure'])\n",
    "        \n",
    "        kmeans=modelOperation.load_model('KMeans')\n",
    "        \n",
    "        clusters=kmeans.predict(data)#drops the first column for cluster prediction\n",
    "        data['clusters']=clusters\n",
    "        clusters=data['clusters'].unique()\n",
    "        result=[] # initialize blank list for storing predicitons\n",
    "        \n",
    "        for i in clusters:\n",
    "            cluster_data= data[data['clusters']==i]\n",
    "            cluster_data = cluster_data.drop(['clusters'],axis=1)\n",
    "            model_name = modelOperation.find_correct_model_file(i)\n",
    "            model = modelOperation.load_model(model_name)\n",
    "            for val in (model.predict(cluster_data.values)):\n",
    "                result.append(val)\n",
    "            \n",
    "        result = pandas.DataFrame(result,columns=['Predictions'])\n",
    "        path=\"Prediction_Output_File/Predictions.csv\"\n",
    "        result.to_csv(\"Prediction_Output_File/Predictions.csv\",header=True) #appends result to prediction file\n",
    "        \n",
    "        return path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943f84d",
   "metadata": {},
   "source": [
    "## Flask  Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from flask import Response\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=['GET'])\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def prediction():\n",
    "    if request.form is not None:\n",
    "        path = request.form['filepath']\n",
    "\n",
    "            # object initialization\n",
    "\n",
    "        pred_validation.prediction_validation()  # calling the prediction_validation function\n",
    "\n",
    "        pred = prediction(path)  # object initialization\n",
    "\n",
    "            # predicting for dataset present in database\n",
    "        path = pred.predictionFromModel()\n",
    "        return Response(\"Prediction File created at %s!!!\" % path)\n",
    "\n",
    "\n",
    "@app.route(\"/train\", methods=['POST'])\n",
    "def trainRouteClient():\n",
    "\n",
    "    try:\n",
    "        if request.json['folderPath'] is not None:\n",
    "            path = request.json['folderPath']\n",
    "            train_valObj = train_validation(path) #object initialization\n",
    "\n",
    "            train_valObj.train_validation()#calling the training_validation function\n",
    "\n",
    "\n",
    "            trainModelObj = trainModel() #object initialization\n",
    "            trainModelObj.trainingModel() #training the model for the files in the table\n",
    "\n",
    "\n",
    "    except ValueError:\n",
    "\n",
    "        return Response(\"Error Occurred! %s\" % ValueError)\n",
    "\n",
    "    except KeyError:\n",
    "\n",
    "        return Response(\"Error Occurred! %s\" % KeyError)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        return Response(\"Error Occurred! %s\" % e)\n",
    "    return Response(\"Training successfull!!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
