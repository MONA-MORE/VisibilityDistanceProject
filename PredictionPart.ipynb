{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab01eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53994f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc734b",
   "metadata": {},
   "source": [
    "## Prediction batch files validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfc59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction_Data_validation:\n",
    "    \n",
    "    def __init__(path):\n",
    "        Batch_Directory = path\n",
    "        schema_path = 'schema_prediction.json'\n",
    "    def valuesFromSchema():\n",
    "        # it will extract all the information from given schema\n",
    "        f = open(schema_path,'r')\n",
    "        dict1 = json.load(f)  # it will return json object containing data in key-value pairs\n",
    "        f.close()\n",
    "        #print(type(dict1))\n",
    "        pattern = dict1['SampleFileName']\n",
    "        LengthOfDateStampInFile = dict1['LengthOfDateStampInFile']\n",
    "        LengthOfTimeStampInFile = dict1['LengthOfTimeStampInFile']\n",
    "        column_names = dict1['ColName']\n",
    "        NumberofColumns = dict1['NumberofColumns']\n",
    "        \n",
    "        return LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, NumberofColumns\n",
    "    \n",
    "    def FileNameRegex():\n",
    "        #regular exp for the filename from training batch files\n",
    "        regex = \"['visibility']+['\\_'']+[\\d_]+[\\d]+\\.csv\"\n",
    "        return regex\n",
    "    def validationFileName(regex,LengthOfDateStampInFile,LengthOfTimeStampInFile):\n",
    "        # file name validation with regex and schema info\n",
    "        \n",
    "        onlyfiles = [f for f in listdir(Batch_Directory)] # this will give all the files at given path\n",
    "        #destination=\"Prediction_files_validated/Good_data\"\n",
    "        #destination2=\"Prediction_files_validated/Bad_data\"\n",
    "        for filename in onlyfiles:\n",
    "                if (re.match(regex, filename)):\n",
    "                    split1 = re.split('.csv', filename)\n",
    "                    split2 = (re.split('_', split1[0]))\n",
    "                    if len(split2[1]) == LengthOfDateStampInFile:\n",
    "                        \n",
    "                        if len(split2[2]) == LengthOfTimeStampInFile:\n",
    "                            shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Good_data\")\n",
    "                        else:\n",
    "                            shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "                    else:\n",
    "                        shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "                else:\n",
    "                    shutil.copy(\"Prediction_Batch_Files/\" + filename, \"Prediction_files_validated/Bad_data\")\n",
    "        \n",
    "    def validateColumnLength(NumberofColumns):\n",
    "        # even if file name is right ,it may happen that no. of cols are not same\n",
    "        # so this function will validate that.\n",
    "        \n",
    "        for file in listdir('Prediction_files_validated/Good_data/'):\n",
    "            csv = pd.read_csv(\"Prediction_files_validated/Good_data/\" + file)\n",
    "            if csv.shape[1] == NumberofColumns:  #shape gives(rows,columns) so index 1\n",
    "                pass\n",
    "            else:\n",
    "                shutil.move(\"Prediction_files_validated/Good_data/\" + file, \"Prediction_files_validated/Bad_data\")\n",
    "                \n",
    "    def deletePredictionFile():\n",
    "\n",
    "        if os.path.exists('Prediction_Output_File/Predictions.csv'):\n",
    "            os.remove('Prediction_Output_File/Predictions.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bbbba1",
   "metadata": {},
   "source": [
    "## Database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf1d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16abe456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dBOperation:           #This class is used for handling all the SQL operations.\n",
    "    def __init__(self): \n",
    "        self.path = \"Prediction_Database/\"\n",
    "        self.badFilePath = \"Training_files_validated/Bad_data\"\n",
    "        self.goodFilePath = \"Training_files_validated/Good_data\"\n",
    "    \n",
    "    def dataBaseConnection(DatabaseName):\n",
    "        # This method creates the database with the given name and \n",
    "        # if Database already exists then opens the connection to the DB.\n",
    "        \n",
    "        conn = sqlite3.connect(self.path+DatabaseName+'.db')\n",
    "        return conn\n",
    "    \n",
    "    def createTableDb(DatabaseName,column_names):\n",
    "        \n",
    "    #This method creates a table in the given database which will be used to insert the Good data\n",
    "        conn = dataBaseConnection(DatabaseName)\n",
    "        for key in column_names.keys():\n",
    "            type = column_names[key]\n",
    "            try:\n",
    "                conn.execute('ALTER TABLE Good_Raw_Data ADD COLUMN \"{column_name}\" {dataType}'.format(column_name=key,dataType=type))\n",
    "            except:\n",
    "                \n",
    "                conn.execute('CREATE TABLE  Good_Raw_Data ({column_name} {dataType})'.format(column_name=key, dataType=type))\n",
    "        conn.close()\n",
    "        \n",
    "    def insertIntoTableGoodData(Database):\n",
    "        conn = dataBaseConnection(Database)\n",
    "        goodFilePath= self.goodFilePath\n",
    "        badFilePath = self..badFilePath\n",
    "        onlyfiles = [f for f in listdir(goodFilePath)]\n",
    "        \n",
    "        for file in onlyfiles:\n",
    "            \n",
    "            with open(goodFilePath+'/'+file, \"r\") as f:\n",
    "                next(f)\n",
    "                reader = csv.reader(f, delimiter=\"\\n\")\n",
    "                for line in enumerate(reader):\n",
    "                    for list_ in (line[1]):\n",
    "                        conn.execute('INSERT INTO Good_Raw_Data values ({values})'.format(values=(list_)))\n",
    "                        conn.commit()\n",
    "                \n",
    "\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "            \n",
    "    def selectingDatafromtableintocsv(Database):\n",
    "        #This method exports the data from Good_Raw_Data table as a CSV file. at a given location.\n",
    "        \n",
    "        \n",
    "        fileName = 'InputFile1.csv'\n",
    "        conn = dataBaseConnection(Database)\n",
    "        sqlSelect = \"SELECT *  FROM Good_Raw_Data\"\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(sqlSelect)\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        # Get the headers of the csv file\n",
    "        headers = [i[0] for i in cursor.description]  #description property will return a list of tuples describing the columns\n",
    "        # 0th index is always a col name in description \n",
    "        with open( fileName, 'w', newline='') as csvFile:\n",
    "            csvFile = csv.writer(csvFile,delimiter=',',lineterminator='\\n')\n",
    "        \n",
    "            csvFile.writerow(headers)   # for single row at a time--to write the field names or col names\n",
    "            csvFile.writerows(results)  # for multiple rows at a time\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26954a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cefcc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_validation:\n",
    "    def __init__(path):\n",
    "        raw_data = Prediction_Data_validation(path)\n",
    "        dBOperation = dBOperation()\n",
    "        \n",
    "    def prediction_validation():\n",
    "        \n",
    "            # extracting values from prediction schema\n",
    "            LengthOfDateStampInFile, LengthOfTimeStampInFile, column_names, noofcolumns = raw_data.valuesFromSchema()\n",
    "            # getting the regex defined to validate filename\n",
    "            regex = raw_data.FileNameRegex()\n",
    "            # validating filename of prediction files\n",
    "            raw_data.validationFileName(regex, LengthOfDateStampInFile, LengthOfTimeStampInFile)\n",
    "            # validating column length in the file\n",
    "            raw_data.validateColumnLength(noofcolumns)\n",
    "            \n",
    "           \n",
    "            # create database with given name, if present open the connection! Create table with columns given in schema\n",
    "            dBOperation.createTableDb('Training', column_names)\n",
    "           \n",
    "            # insert csv files in the table\n",
    "            dBOperation.insertIntoTableGoodData('Training')\n",
    "            \n",
    "\n",
    "            # export data in table to csvfile\n",
    "            dBOperation.selectingDatafromtableintocsv('Training')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40ba34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec111fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class modelOperation:\n",
    "    \n",
    "    model_dir = \"D:/cdac/CDAC_PROJECT/Untitled Folder/model\"\n",
    "    \n",
    "    def save_model(model,filename):\n",
    "        path = os.path.join(\"D:/cdac/CDAC_PROJECT/Untitled Folder/model\",filename)  #create seperate directory for each cluster\n",
    "        if os.path.isdir(path):   #remove previously existing models for each clusters\n",
    "            shutil.rmtree(model_dir)\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            os.makedirs(path) \n",
    "            with open(path +'/' + filename+'.sav','wb') as f:\n",
    "                \n",
    "                pickle.dump(model, f)\n",
    "        return 'success'\n",
    "    \n",
    "    def load_model(filename):\n",
    "        with open(model_dir + filename + '/' + filename + '.sav','rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def find_correct_model_file(cluster_number):\n",
    "        \n",
    "        cluster_number= cluster_number\n",
    "        folder_name= model_dir\n",
    "        list_of_model_files = []\n",
    "        list_of_files = os.listdir(folder_name)\n",
    "        for file in list_of_files:\n",
    "            try:\n",
    "                if (file.index(str(cluster_number))!=-1):\n",
    "                    model_name=file\n",
    "            except:\n",
    "                continue\n",
    "        model_name=model_name.split('.')[0]\n",
    "        return model_name\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8722f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction:\n",
    "    def __init__(path):\n",
    "        pred_data_val = Prediction_Data_validation(path)\n",
    "    \n",
    "    def predictionFromModel():\n",
    "        data = preprocessor.dropUnnecessaryCol(data,['DATE','Precip','WETBULBTEMPF','DewPointTempF','StationPressure'])\n",
    "        \n",
    "        kmeans=modelOperation.load_model('KMeans')\n",
    "        \n",
    "        clusters=kmeans.predict(data)#drops the first column for cluster prediction\n",
    "        data['clusters']=clusters\n",
    "        clusters=data['clusters'].unique()\n",
    "        result=[] # initialize blank list for storing predicitons\n",
    "        \n",
    "        for i in clusters:\n",
    "            cluster_data= data[data['clusters']==i]\n",
    "            cluster_data = cluster_data.drop(['clusters'],axis=1)\n",
    "            model_name = modelOperation.find_correct_model_file(i)\n",
    "            model = modelOperation.load_model(model_name)\n",
    "            for val in (model.predict(cluster_data.values)):\n",
    "                result.append(val)\n",
    "            \n",
    "        result = pandas.DataFrame(result,columns=['Predictions'])\n",
    "        path=\"Prediction_Output_File/Predictions.csv\"\n",
    "        result.to_csv(\"Prediction_Output_File/Predictions.csv\",header=True) #appends result to prediction file\n",
    "        \n",
    "        return path\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419be711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
